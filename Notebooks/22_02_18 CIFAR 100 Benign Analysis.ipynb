{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 443.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:14<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 4.612 | Train Acc: 1.125% |Test Loss: 4.612 | Test Acc: 0.983% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Generating Empty Aggregator to be loaded \n",
    "\n",
    "setting = 'FedAvg'\n",
    "\n",
    "if setting == 'FedEM':\n",
    "    nL = 3\n",
    "else:\n",
    "    nL = 1\n",
    "\n",
    "adv_mode = False    \n",
    "    \n",
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar100\"\n",
    "args_.method = setting\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= nL\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/dummy/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_, num_user=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling Dataset from Clients\n",
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].test_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "try:\n",
    "    data_y = torch.stack(data_y)        \n",
    "except:\n",
    "    data_y = torch.FloatTensor(data_y) \n",
    "    \n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Weights\n",
    "num_models = 50\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "if setting == 'local':\n",
    "\n",
    "    if adv_mode:\n",
    "        args_.save_path ='weights/cifar100/feddef_l2/local/'\n",
    "        aggregator.load_state(args_.save_path)\n",
    "\n",
    "        model_weights = []\n",
    "        weights = np.load('weights/cifar100/feddef_l2/local/train_client_weights.npy')\n",
    "    else:\n",
    "        args_.save_path ='weights/cifar100/first_test/local/'\n",
    "        aggregator.load_state(args_.save_path)\n",
    "\n",
    "        model_weights = []\n",
    "        weights = np.load('weights/cifar100/first_test/local/train_client_weights.npy')\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        new_model = copy.deepcopy(aggregator.clients[i].learners_ensemble.learners[0].model)\n",
    "        new_model.eval()\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedAvg':\n",
    "    \n",
    "    \n",
    "    if adv_mode:\n",
    "        args_.save_path = 'weights/cifar100/feddef_l2/fedavg/'\n",
    "    else:\n",
    "        args_.save_path = 'weights/cifar100/first_test/fedavg/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "    \n",
    "    if adv_mode:\n",
    "        weights = np.load('weights/cifar100/feddef_l2/fedavg/train_client_weights.npy')\n",
    "    else:\n",
    "        weights = np.load('weights/cifar100/first_test/fedavg/train_client_weights.npy')\n",
    "    \n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0[0]*weights_h[0][key] \n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedEM':\n",
    "    \n",
    "    if adv_mode:\n",
    "        args_.save_path = 'weights/cifar100/feddef_l2/fedEM/'\n",
    "    else:\n",
    "        args_.save_path = 'weights/cifar100/first_test/fedEM/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "    if adv_mode:\n",
    "        weights = np.load('weights/cifar100/feddef_l2/fedEM/train_client_weights.npy')\n",
    "    else:\n",
    "        weights = np.load('weights/cifar100/first_test/fedEM/train_client_weights.npy')\n",
    "\n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights2 = aggregator.clients[0].learners_ensemble.learners_weights\n",
    "len(aggregator.clients[0].learners_ensemble.learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide models into 5 sub-groups and run \n",
    "num_groups = 5\n",
    "vic_dic ={}\n",
    "sub_user = 10\n",
    "\n",
    "for i in range(num_groups):\n",
    "    vic_dic[i] = range(i*sub_user, (i+1)*sub_user)\n",
    "\n",
    "upper_logs = []\n",
    "\n",
    "for j in range(num_groups):\n",
    "    logs_adv = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adv_dict = {}\n",
    "        adv_dict['orig_acc_transfers'] = None\n",
    "        adv_dict['orig_similarities'] = None\n",
    "        adv_dict['adv_acc_transfers'] = None\n",
    "        adv_dict['adv_similarities_target'] = None\n",
    "        adv_dict['adv_similarities_untarget'] = None\n",
    "        adv_dict['adv_target'] = None\n",
    "        adv_dict['adv_miss'] = None\n",
    "        adv_dict['metric_alignment'] = None\n",
    "        adv_dict['ib_distance_legit'] = None\n",
    "        adv_dict['ib_distance_adv'] = None\n",
    "\n",
    "        logs_adv += [adv_dict]\n",
    "    upper_logs += [logs_adv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n",
      "\t Adv idx: 8\n",
      "\t Adv idx: 9\n",
      "\t Adv idx: 10\n",
      "\t Adv idx: 11\n",
      "\t Adv idx: 12\n",
      "\t Adv idx: 13\n",
      "\t Adv idx: 14\n",
      "\t Adv idx: 15\n",
      "\t Adv idx: 16\n",
      "\t Adv idx: 17\n",
      "\t Adv idx: 18\n",
      "\t Adv idx: 19\n",
      "\t Adv idx: 20\n",
      "\t Adv idx: 21\n",
      "\t Adv idx: 22\n",
      "\t Adv idx: 23\n",
      "\t Adv idx: 24\n",
      "\t Adv idx: 25\n",
      "\t Adv idx: 26\n",
      "\t Adv idx: 27\n",
      "\t Adv idx: 28\n",
      "\t Adv idx: 29\n",
      "\t Adv idx: 30\n",
      "\t Adv idx: 31\n",
      "\t Adv idx: 32\n",
      "\t Adv idx: 33\n",
      "\t Adv idx: 34\n",
      "\t Adv idx: 35\n",
      "\t Adv idx: 36\n",
      "\t Adv idx: 37\n",
      "\t Adv idx: 38\n",
      "\t Adv idx: 39\n",
      "\t Adv idx: 40\n",
      "\t Adv idx: 41\n",
      "\t Adv idx: 42\n",
      "\t Adv idx: 43\n",
      "\t Adv idx: 44\n",
      "\t Adv idx: 45\n",
      "\t Adv idx: 46\n",
      "\t Adv idx: 47\n",
      "\t Adv idx: 48\n",
      "\t Adv idx: 49\n"
     ]
    }
   ],
   "source": [
    "rec_dict = {}\n",
    "\n",
    "for i in range(num_groups):\n",
    "    # Run Measurements for both targetted and untargeted analysis\n",
    "    victim_idxs = vic_dic[i]\n",
    "    custom_batch_size = 500\n",
    "\n",
    "\n",
    "    for adv_idx in victim_idxs:\n",
    "        print(\"\\t Adv idx:\", adv_idx)\n",
    "\n",
    "        dataloader = load_client_data(clients = clients, c_id = adv_idx, mode = 'test') # or test/train\n",
    "\n",
    "        batch_size = min(custom_batch_size, dataloader.y_data.shape[0])\n",
    "        \n",
    "        \n",
    "        dataloader = load_client_data(clients = clients, c_id = adv_idx, mode = 'test') # or test/train\n",
    "    \n",
    "        batch_size = min(custom_batch_size, dataloader.y_data.shape[0])\n",
    "        batch_size_recs[adv_idx,:] *= batch_size\n",
    "\n",
    "        t1 = Transferer(models_list=models_test, dataloader=dataloader)\n",
    "        t1.generate_victims(victim_idxs)\n",
    "\n",
    "        # Perform Attacks\n",
    "        t1.atk_params = PGD_Params()\n",
    "        t1.atk_params.set_params(batch_size=batch_size, iteration = 10,\n",
    "                       target = 3, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                       step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "\n",
    "\n",
    "\n",
    "        t1.generate_advNN(adv_idx)\n",
    "        t1.generate_xadv(atk_type = \"pgd\")\n",
    "        t1.send_to_victims(victim_idxs)\n",
    "\n",
    "        # Log Performance\n",
    "        upper_logs[i][adv_idx]['orig_acc_transfers'] = copy.deepcopy(t1.orig_acc_transfers)\n",
    "        upper_logs[i][adv_idx]['orig_similarities'] = copy.deepcopy(t1.orig_similarities)\n",
    "        upper_logs[i][adv_idx]['adv_acc_transfers'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "        upper_logs[i][adv_idx]['adv_similarities_target'] = copy.deepcopy(t1.adv_similarities)        \n",
    "        upper_logs[i][adv_idx]['adv_target'] = copy.deepcopy(t1.adv_target_hit)\n",
    "\n",
    "        # Miss attack\n",
    "        t1.atk_params.set_params(batch_size=batch_size, iteration = 20,\n",
    "                       target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                       step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "        t1.generate_xadv(atk_type = \"pgd\")\n",
    "        t1.send_to_victims(victim_idxs)\n",
    "        upper_logs[i][adv_idx]['adv_miss'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "        upper_logs[i][adv_idx]['adv_similarities_untarget'] = copy.deepcopy(t1.adv_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['orig_acc_transfers','orig_similarities','adv_acc_transfers','adv_similarities_target',\n",
    "           'adv_similarities_untarget','adv_target','adv_miss']\n",
    "\n",
    "adv_targets = []\n",
    "adv_misses = []\n",
    "orig_accs = []\n",
    "\n",
    "for i in range(num_groups):\n",
    "    victim_idxs = vic_dic[i]\n",
    "    \n",
    "    orig_acc = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "    orig_sim = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "    adv_acc = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "    adv_sim_target = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "    adv_sim_untarget = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "    adv_target = np.zeros([len(victim_idxs),len(victim_idxs)])\n",
    "    adv_miss = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "\n",
    "    for adv_idx in range(len(victim_idxs)):\n",
    "        for victim in range(len(victim_idxs)):\n",
    "            orig_acc[adv_idx,victim] = upper_logs[i][victim_idxs[adv_idx]][metrics[0]][victim_idxs[victim]].data.tolist()\n",
    "            orig_sim[adv_idx,victim] = upper_logs[i][victim_idxs[adv_idx]][metrics[1]][victim_idxs[victim]].data.tolist()\n",
    "            adv_acc[adv_idx,victim] = upper_logs[i][victim_idxs[adv_idx]][metrics[2]][victim_idxs[victim]].data.tolist()\n",
    "            adv_sim_target[adv_idx,victim] = upper_logs[i][victim_idxs[adv_idx]][metrics[3]][victim_idxs[victim]].data.tolist()\n",
    "            adv_sim_untarget[adv_idx,victim] = upper_logs[i][victim_idxs[adv_idx]][metrics[4]][victim_idxs[victim]].data.tolist()\n",
    "            adv_target[adv_idx,victim] = upper_logs[i][victim_idxs[adv_idx]][metrics[5]][victim_idxs[victim]].data.tolist()\n",
    "            adv_miss[adv_idx,victim] = upper_logs[i][victim_idxs[adv_idx]][metrics[6]][victim_idxs[victim]].data.tolist()\n",
    "            \n",
    "    adv_targets += [avg_nondiag(adv_target)]\n",
    "    adv_misses += [avg_nondiag(adv_miss)]\n",
    "    orig_accs += [np.mean(np.diagonal(orig_acc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv_target -- mean: 0.4880338102579117 sd: 0.025082812817303558\n",
      "adv_miss -- mean: 0.001666666753590107 sd: 0.0009128709767852059\n",
      "orig_acc -- mean: 0.6778333747386933 sd: 0.004170000932490291\n"
     ]
    }
   ],
   "source": [
    "print('adv_target --', 'mean:', np.mean(adv_targets), 'sd:', np.std(adv_targets))\n",
    "print('adv_miss --', 'mean:', np.mean(adv_misses), 'sd:', np.std(adv_misses))\n",
    "print('orig_acc --', 'mean:', np.mean(orig_accs), 'sd:', np.std(orig_accs))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Local Benign\n",
    "adv_target -- mean: 0.011034660016496976 sd: 0.006705277797261389\n",
    "adv_miss -- mean: 0.08579630085784529 sd: 0.0034904338409654893\n",
    "orig_acc -- mean: 0.10566667236387728 sd: 0.00751664847795914\n",
    "\n",
    "# FedAvg Benign\n",
    "adv_target -- mean: 0.6134949767589569 sd: 0.017525302455574102\n",
    "adv_miss -- mean: 0.001166666727513075 sd: 0.0008498366299212294\n",
    "orig_acc -- mean: 0.6778333747386933 sd: 0.004170000932490291\n",
    "\n",
    "# FedEM Benign\n",
    "adv_target -- mean: 0.16861164377381405 sd: 0.03138360212577547\n",
    "adv_miss -- mean: 0.006759259646965399 sd: 0.003502889458715223\n",
    "orig_acc -- mean: 0.3331666848063469 sd: 0.0380912657871654\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66],\n",
       "       [0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73],\n",
       "       [0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67],\n",
       "       [0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66],\n",
       "       [0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66],\n",
       "       [0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76],\n",
       "       [0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68],\n",
       "       [0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67],\n",
       "       [0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59],\n",
       "       [0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trial: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5139/2149045461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_adv_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_nn_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         base_ep_legit, victim_eps_legit = t1.legitimate_direction(batch_size=batch_size, ep_granularity = 0.5, \n\u001b[0m\u001b[1;32m     37\u001b[0m                                                                   rep_padding = 200, new_point = True,print_res = False)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FedEM/transfer_attacks/Boundary_Transferer.py\u001b[0m in \u001b[0;36mlegitimate_direction\u001b[0;34m(self, batch_size, ep_granularity, rep_padding, new_point, print_res)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# Select point of baseline comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_point\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Select set of comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FedEM/transfer_attacks/Boundary_Transferer.py\u001b[0m in \u001b[0;36mselect_data_point\u001b[0;34m(self, explore_set_size)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnidx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mcorrect_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnidx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtemp_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "batch_size = 50\n",
    "adv_idxs = [0,10,20,30,40]\n",
    "group_list = [0,1,2,3,4]\n",
    "\n",
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)\n",
    "num_model2 = 3\n",
    "\n",
    "dmls = []\n",
    "dma = []\n",
    "dmae = []\n",
    "\n",
    "for i in group_list:\n",
    "    victim_idxs = vic_dic[i]\n",
    "    adv_idx = adv_idxs[i]\n",
    "\n",
    "    dists_measure_legit = np.zeros([num_trials, num_model2-1])\n",
    "    dists_measure_adv = np.zeros([num_trials, num_model2-1])\n",
    "    dists_measure_adv_ensemble = np.zeros([num_trials, num_model2-1])\n",
    "\n",
    "\n",
    "#     for j in range(num_model2):\n",
    "    for i in range(num_trials):\n",
    "        print(\"num_trial:\", i)\n",
    "        t1 = Boundary_Transferer(models_list=models_test[victim_idxs[0]:(victim_idxs[-1]+1)], dataloader=dataloader)\n",
    "        t1.base_nn_idx = 0\n",
    "        t1.victim_idx = list(range(1, num_model2))\n",
    "\n",
    "        t1.atk_params = PGD_Params()\n",
    "        t1.atk_params.set_params(batch_size=500, iteration = 30,\n",
    "                       target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                       step_size = 0.05, step_norm = \"inf\", eps = 3, eps_norm = 2)\n",
    "        t1.set_adv_NN(t1.base_nn_idx)\n",
    "\n",
    "        base_ep_legit, victim_eps_legit = t1.legitimate_direction(batch_size=batch_size, ep_granularity = 0.5, \n",
    "                                                                  rep_padding = 200, new_point = True,print_res = False)\n",
    "\n",
    "        base_ep_adv, victim_eps_adv = t1.adversarial_direction(ep_granularity = 0.5, \n",
    "                                                                  rep_padding = 200, new_point = False,print_res = False)\n",
    "\n",
    "        idx = 0\n",
    "        for key, value in victim_eps_legit.items():\n",
    "            dists_measure_legit[i,idx] = np.abs(base_ep_legit-value)\n",
    "            idx+=1\n",
    "\n",
    "        idx = 0\n",
    "        for key, value in victim_eps_adv.items():\n",
    "            dists_measure_adv[i,idx] = np.abs(base_ep_adv - value)\n",
    "            idx+=1\n",
    "                \n",
    "    dmls += [dists_measure_legit]\n",
    "    dma += [dists_measure_adv]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('legit mean:', np.mean(np.average(dmls,axis=1)))\n",
    "print('legit std:', np.std(np.average(dmls,axis=1))\n",
    "print('adv mean:',np.mean(np.average(dma,axis=1)))\n",
    "print('adv std:', np.std(np.average(dma,axis=1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "40 clients analysis\n",
    "\n",
    "# Local Benign\n",
    "adv_target: 0.00789300038670309\n",
    "adv_miss: 0.07762286755948877\n",
    "orig_acc: 0.3275000169873238\n",
    "\n",
    "# FedAvg Benign\n",
    "adv_target: 0.5037565186619759\n",
    "adv_miss: 0.006875000358559191\n",
    "orig_acc: 0.4450000233948231\n",
    "\n",
    "# FedEM Benign\n",
    "adv_target: 0.10303049261610095\n",
    "adv_miss: 0.02349893300411984\n",
    "orig_acc: 0.35541668608784677\n",
    "\n",
    "G = 0.5\n",
    "\n",
    "# Local Adv\n",
    "adv_target: 0.008927402751615797\n",
    "adv_miss: 0.08478098753063629\n",
    "orig_acc: 0.3358333520591259\n",
    "\n",
    "# FedAvg Adv\n",
    "adv_target: 0.08916715495288371\n",
    "adv_miss: 0.09479167168028653\n",
    "orig_acc: 0.4104166876524687\n",
    "\n",
    "# FedEM Adv\n",
    "adv_target: 0.040357855753973125\n",
    "adv_miss: 0.10907586068750764\n",
    "orig_acc: 0.3085416827350855\n",
    "\n",
    "\n",
    "G = 0.2\n",
    "\n",
    "# Local Adv\n",
    "adv_target: 0.00920581775299536\n",
    "adv_miss: 0.08648504754946304\n",
    "orig_acc: 0.33979168608784677\n",
    "\n",
    "# FedAvg\n",
    "adv_target: 0.21372711043804885\n",
    "adv_miss: 0.03729166861157864\n",
    "orig_acc: 0.42791669219732287\n",
    "\n",
    "# FedEM Adv\n",
    "adv_target: 0.09712168363233407\n",
    "adv_miss: 0.07686966224692952\n",
    "orig_acc: 0.3385416831821203"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
